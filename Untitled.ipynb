{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_dataset(trainsample=5000, testsample=500):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    trainindex = random.sample(list(range(x_train.shape[0])), trainsample)\n",
    "    \n",
    "    return x_train[trainindex,:,:,:], y_train[trainindex,:], x_test[0:testsample,:,:,:], y_test[0:testsample,:], x_test[testsample:testsample*2,:,:,:], y_test[testsample:testsample*2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_model(input_shape):\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 10\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(16, 64), (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(random.randint(0,5)*0.1))\n",
    "\n",
    "    model.add(Conv2D(random.randint(16, 64), (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(16, 64), (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(random.randint(0,5)*0.1))\n",
    "\n",
    "    model.add(Conv2D(random.randint(64, 128), (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(64, 128), (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(random.randint(0,5)*0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=90,\n",
    "                 width_shift_range=0.1, height_shift_range=0.1,\n",
    "                 horizontal_flip=True)\n",
    "batch_size = 32 \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 19s 121ms/step - loss: 2.9670 - accuracy: 0.2194 - val_loss: 4.9614 - val_accuracy: 0.2280\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 18s 116ms/step - loss: 2.4961 - accuracy: 0.2496 - val_loss: 2.3771 - val_accuracy: 0.3220\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 19s 124ms/step - loss: 2.2443 - accuracy: 0.2828 - val_loss: 1.9976 - val_accuracy: 0.3360\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 19s 123ms/step - loss: 2.1953 - accuracy: 0.2989 - val_loss: 2.0436 - val_accuracy: 0.3280\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 19s 121ms/step - loss: 2.1045 - accuracy: 0.3265 - val_loss: 1.9779 - val_accuracy: 0.3440\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 19s 125ms/step - loss: 2.0465 - accuracy: 0.3402 - val_loss: 2.6574 - val_accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 18s 116ms/step - loss: 1.9794 - accuracy: 0.3555 - val_loss: 2.3093 - val_accuracy: 0.3340\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 19s 122ms/step - loss: 1.9345 - accuracy: 0.3669 - val_loss: 2.2232 - val_accuracy: 0.2860\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 19s 119ms/step - loss: 1.9114 - accuracy: 0.3599 - val_loss: 2.1755 - val_accuracy: 0.3720\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 20s 126ms/step - loss: 1.8878 - accuracy: 0.3661 - val_loss: 1.7727 - val_accuracy: 0.4180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13f574e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1, y_train1, x_val1, y_val1, x_test1, y_test1 = load_dataset()\n",
    "model1=create_random_model(input_shape=x_train1.shape[1:])\n",
    "datagen.fit(x_train1)\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model1.fit_generator(datagen.flow(x_train1, y_train1, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train1.shape[0] // batch_size,epochs=10,\\\n",
    "                    verbose=1,validation_data=(x_test1,y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 20s 126ms/step - loss: 2.7288 - accuracy: 0.2355 - val_loss: 4.6400 - val_accuracy: 0.1680\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 19s 123ms/step - loss: 2.3364 - accuracy: 0.2625 - val_loss: 3.1483 - val_accuracy: 0.1980\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 19s 121ms/step - loss: 2.1732 - accuracy: 0.3058 - val_loss: 2.0575 - val_accuracy: 0.3640\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 19s 121ms/step - loss: 2.0563 - accuracy: 0.3325 - val_loss: 2.0220 - val_accuracy: 0.3580\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 19s 122ms/step - loss: 2.0104 - accuracy: 0.3416 - val_loss: 2.3301 - val_accuracy: 0.3120\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 18s 113ms/step - loss: 1.9529 - accuracy: 0.3506 - val_loss: 2.4206 - val_accuracy: 0.3020\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 23s 147ms/step - loss: 1.8730 - accuracy: 0.3696 - val_loss: 1.9082 - val_accuracy: 0.4100\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 19s 124ms/step - loss: 1.8262 - accuracy: 0.3835 - val_loss: 2.3623 - val_accuracy: 0.3100\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 20s 127ms/step - loss: 1.7775 - accuracy: 0.3935 - val_loss: 2.5220 - val_accuracy: 0.3040\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 21s 132ms/step - loss: 1.7245 - accuracy: 0.4096 - val_loss: 1.6721 - val_accuracy: 0.4460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1434659e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2, y_train2, x_val2, y_val2, x_test2, y_test2 = load_dataset()\n",
    "model2=create_random_model(input_shape=x_train2.shape[1:])\n",
    "datagen.fit(x_train2)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model2.fit_generator(datagen.flow(x_train2, y_train2, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train2.shape[0] // batch_size,epochs=10,\\\n",
    "                    verbose=1,validation_data=(x_val2,y_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 22s 142ms/step - loss: 2.6096 - accuracy: 0.2434 - val_loss: 2.8528 - val_accuracy: 0.3020\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 20s 131ms/step - loss: 2.2520 - accuracy: 0.2888 - val_loss: 2.1319 - val_accuracy: 0.3240\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 21s 135ms/step - loss: 2.1425 - accuracy: 0.3090 - val_loss: 1.9478 - val_accuracy: 0.3600\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 21s 135ms/step - loss: 2.0579 - accuracy: 0.3207 - val_loss: 2.9220 - val_accuracy: 0.2460\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 1.9515 - accuracy: 0.3506 - val_loss: 2.3432 - val_accuracy: 0.3120\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 22s 140ms/step - loss: 1.8802 - accuracy: 0.3722 - val_loss: 2.3468 - val_accuracy: 0.3040\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 20s 128ms/step - loss: 1.8251 - accuracy: 0.3814 - val_loss: 1.8891 - val_accuracy: 0.3760\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 21s 134ms/step - loss: 1.7981 - accuracy: 0.3921 - val_loss: 1.9272 - val_accuracy: 0.3860\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 22s 138ms/step - loss: 1.7494 - accuracy: 0.4040 - val_loss: 2.3531 - val_accuracy: 0.3400\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 21s 134ms/step - loss: 1.7045 - accuracy: 0.4213 - val_loss: 2.0213 - val_accuracy: 0.3900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16f2ffe10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train3, y_train3, x_val3, y_val3, x_test3, y_test3 = load_dataset()\n",
    "model3=create_random_model(input_shape=x_train3.shape[1:])\n",
    "datagen.fit(x_train3)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model3.fit_generator(datagen.flow(x_train3, y_train3, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train3.shape[0] // batch_size,epochs=10,\\\n",
    "                    verbose=1,validation_data=(x_val3,y_val3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "from abc import abstractmethod\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "\n",
    "class Member:\n",
    "    \"\"\"\n",
    "    Representation of a single keras model member (Base-Learner) of an Ensemble\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None, keras_model=None, train_batches=None, val_batches=None,\n",
    "                 submission_probs=None, keras_modelpath=None, keras_kwargs={}):\n",
    "        \"\"\"\n",
    "        Constructor of a Keras Ensemble Member for a Binary Classification (Image Recognition) Task.\n",
    "        Internal class probabilities are calculates based on ImageDataGenerators.\n",
    "        If you wish to provide these directly, use `from_probs` constructor\n",
    "        Args:\n",
    "            name: name of the model. Must be unique.\n",
    "            model: the (pre-trained) keras model. Or provide `keras_modelpath` instead.\n",
    "            train_batches: an instance of Keras `ImageDataGenerator` for training the Meta-Learner\n",
    "            val_batches: an instance of Keras `ImageDataGenerator` for validating the Meta-Learner\n",
    "            submission_probs: the submission prediction probabilities\n",
    "            keras_modelpath: path to load keras model from (if `model` argument is None)\n",
    "            keras_kwargs: kwargs for keras `load_model` (if `model` argument is None)\n",
    "        \"\"\"\n",
    "        assert(name is not None)\n",
    "        self.name = name\n",
    "        self.model = keras_model\n",
    "        self.submission_probs = submission_probs\n",
    "        # Initialize Params\n",
    "        self.val_probs = None\n",
    "        self.train_probs = None\n",
    "        self.val_classes = None\n",
    "        self.train_classes = None\n",
    "        if (keras_model is None) and (keras_modelpath is not None):\n",
    "            self.load_kerasmodel(self.keras_modelpath, self.keras_kwargs)\n",
    "        if val_batches is not None:\n",
    "            self._calculate_val_predictions(val_batches)\n",
    "        if train_batches is not None:\n",
    "            self._calculate_train_predictions(train_batches)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Member: \" + self.name + \">\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Member: \" + self.name\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        c1 = self.name == other.name\n",
    "        c2 = _compare_arrays(self.train_classes, other.train_classes)\n",
    "        c3 = _compare_arrays(self.val_classes, other.val_classes)\n",
    "        c4 = _compare_arrays(self.val_probs, other.val_probs)\n",
    "        c5 = _compare_arrays(self.train_probs, other.train_probs)\n",
    "        return c1 and c2 and c3 and c4 and c5\n",
    "\n",
    "    @classmethod\n",
    "    def from_probs(cls, name, train_probs, train_classes, val_probs, val_classes, submission_probs):\n",
    "        \"\"\"\n",
    "        Constructor based on class probabilities, not on `ImageDataGenerator`.\n",
    "        Useful if one wants to calculated the keras model's probabilities idependently.\n",
    "        Args:\n",
    "            name: name of the model. Must be unique.\n",
    "            train_probs: probabilities of positive class for training the Meta-Learner\n",
    "            train_classes: ground truth (classes) for training the Meta-Learner\n",
    "            val_probs: probabilities of positive class for validating the Meta-Learner\n",
    "            val_classes: ground truth (classes) for validating the Meta-Learner\n",
    "            submission_probs: the submission prediction probabilities\n",
    "        Returns:\n",
    "            a Member object\n",
    "        \"\"\"\n",
    "        member = cls(name=name)\n",
    "        member.train_probs = np.array(train_probs)\n",
    "        member.train_classes = np.array(train_classes)\n",
    "        member.val_probs = np.array(val_probs)\n",
    "        member.val_classes = np.array(val_classes)\n",
    "        member.submission_probs = np.array(submission_probs)\n",
    "        return member\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, folder=None):\n",
    "        \"\"\"Loads base-learner from directory\n",
    "        Args:\n",
    "            folder: directory where member is saved\n",
    "        Returns:\n",
    "            loaded Member object\n",
    "        \"\"\"\n",
    "        path = os.path.join(folder, \"member.joblib\")\n",
    "        member = joblib.load(path)\n",
    "        return member\n",
    "\n",
    "    def _calculate_predictions(self, batches):  # TODO: call automatically for dirichlet ensemble\n",
    "        if hasattr(batches, 'shuffle'):\n",
    "            batches.reset()\n",
    "            batches.shuffle = False\n",
    "        return self.model.predict_generator(batches, steps=(batches.n // batches.batch_size) + 1, verbose=1)\n",
    "\n",
    "    def _calculate_val_predictions(self, val_batches):  # TODO: call automatically for dirichlet ensemble\n",
    "        self.val_probs = self._calculate_predictions(val_batches)\n",
    "        if hasattr(val_batches, 'classes'):\n",
    "            self.val_classes = np.array(val_batches.classes)\n",
    "        elif hasattr(val_batches, 'y'):\n",
    "            self.val_classes = np.array(val_batches.y)\n",
    "        else:\n",
    "            raise(\"No known class in data batch\")\n",
    "        return self.val_probs\n",
    "\n",
    "    def _calculate_train_predictions(self, train_batches):  # TODO: call automatically for dirichlet ensemble\n",
    "        self.train_probs = self._calculate_predictions(train_batches)\n",
    "        if hasattr(train_batches, 'classes'):\n",
    "            self.train_classes = np.array(train_batches.classes)\n",
    "        elif hasattr(train_batches, 'y'):\n",
    "            self.train_classes = np.array(train_batches.y)\n",
    "        else:\n",
    "            raise(\"No known class in data batch\")\n",
    "        return self.train_probs\n",
    "\n",
    "    def load_kerasmodel(self, keras_modelpath=None, keras_kwargs={}):\n",
    "        \"\"\"\n",
    "        Utility method for loading Keras model\n",
    "        Args:\n",
    "            keras_modelpath: path to keras model\n",
    "            keras_kwargs: arguments for keras `load_model`\n",
    "        \"\"\"\n",
    "        if keras_kwargs is None:\n",
    "            keras_kwargs = {}\n",
    "        self.model = load_model(keras_modelpath, **keras_kwargs)\n",
    "        print(\"Keras Model Loaded:\", keras_modelpath)\n",
    "        return self.model\n",
    "\n",
    "    def save(self, folder=\"./premodels/\", save_kerasmodel=False):\n",
    "        \"\"\"\n",
    "        Saves member object to folder\n",
    "        Args:\n",
    "            folder: the folder where models should be saved to. Create if not exists.\n",
    "            save_kerasmodel: if it should save the keras model as part of object.\n",
    "                Recommendation is to load keras separately with method `load_kerasmodel`\n",
    "        \"\"\"\n",
    "        if folder[-1] != os.sep:\n",
    "            folder += os.sep\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        if not os.path.exists(folder + self.name):\n",
    "            os.mkdir(folder + self.name)\n",
    "        if save_kerasmodel:\n",
    "            joblib.dump(self, os.path.join(folder + self.name, \"member.joblib\"))\n",
    "        else:\n",
    "            temp = self.model\n",
    "            self.model = None  # Remove Keras model from variable\n",
    "            joblib.dump(self, os.path.join(folder + self.name, \"member.joblib\"))\n",
    "            self.model = temp\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    \"\"\"Base Ensemble Definition.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_member(self, member):\n",
    "        \"\"\"\n",
    "        Adds a model Member to the Ensemble\n",
    "\n",
    "        Args:\n",
    "            member: the model Member\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fit method to provided ensemble members\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict using fitted ensemble members\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DirichletEnsemble(Ensemble):\n",
    "    \"\"\"\n",
    "    Representation of an ensemble of Keras Models for a Binary Classification Task.\n",
    "    It weights the ensemble members optimizing the AUC-Score based on a validation dataset.\n",
    "    The weight optimization search is performed with randomized search based on the dirichlet distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, N=10000):\n",
    "        \"\"\"\n",
    "        Constructor of a Keras Ensemble Binary Classifier\n",
    "        Args:\n",
    "            val_classes: classes of validation dataset, from which the enseble weights will be calculated\n",
    "            N: the number of times weights should be (randomly) tried out, sampled from a dirichlet distribution\n",
    "        \"\"\"\n",
    "        self.n = N\n",
    "        # Initialize Parameters:\n",
    "        self.members = []\n",
    "        self.bestweights = []\n",
    "        self.probabilities = None\n",
    "        self._nmembers = 0\n",
    "        self.bestauc = 0\n",
    "        self.fitted = False\n",
    "\n",
    "    def add_member(self, member):\n",
    "        \"\"\"\n",
    "        Adds a ensemble Member to the Stack\n",
    "        Args:\n",
    "            member: an instance of class `Member`\n",
    "        \"\"\"\n",
    "        self.members.append(member)\n",
    "        self._nmembers += 1\n",
    "\n",
    "    def fit(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Calculates ensemble weights, optimizing the AUC Binary Classification Metric using randomized\n",
    "        search with the dirichlet distribution.\n",
    "        \"\"\"\n",
    "        assert(len(self.members) > 1)\n",
    "        val_classes = self.members[0].val_classes\n",
    "\n",
    "        aucbest = 0\n",
    "        rsbest = None\n",
    "        for i in range(self.n):\n",
    "            rs = np.random.dirichlet(np.ones(self._nmembers), size=1)[0]\n",
    "            preds = np.sum(np.array([self.members[i].val_probs * rs[i] for i in range(self._nmembers)]), axis=0)\n",
    "            auc = metrics.roc_auc_score(val_classes, preds)\n",
    "            if auc > aucbest:\n",
    "                if verbose:\n",
    "                    print(auc, i, rs)  # TODO: Proper logging\n",
    "                aucbest = auc\n",
    "                rsbest = rs\n",
    "        self.bestweights = rsbest\n",
    "        self.bestauc = aucbest\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Returns the weighed probabilities of the ensemble members\n",
    "        Returns:\n",
    "            the predicted probabilities as np.array\n",
    "        \"\"\"\n",
    "        self.probabilities = np.sum(np.array([self.bestweights[i] * self.members[i].submission_probs\n",
    "                                              for i in range(self._nmembers)]), axis=0)\n",
    "        return self.probabilities\n",
    "\n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Prints information about the ensemble members and its weights as well as single and ensemble AUC performance\n",
    "        on validation dataset.\n",
    "        \"\"\"\n",
    "        for i in range(self._nmembers):\n",
    "            member = self.members[i]\n",
    "            auc = metrics.roc_auc_score(member.val_classes, member.val_probs)\n",
    "            print(self.members[i].name, \"- Weight:\", self.bestweights[i], \"- Single AUC:\", auc)\n",
    "        print(\"DirichletEnsemble AUC:\", self.bestauc)\n",
    "        return\n",
    "\n",
    "\n",
    "class StackEnsemble(Ensemble):\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\"\n",
    "        Constructor of a Stacking Ensemble, with Keras Models as Base-Learners.\n",
    "        It supports by now only keras binary classifiers. It constructs a meta-learner that predicts\n",
    "        the probability of the positive class as a regression problem. \n",
    "        Args:\n",
    "            model: ensemble model which should serve as meta-model. Sklearn RandomForestRegressor per default.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            self.model = RandomForestRegressor(verbose=1, n_estimators=100, max_depth=3)\n",
    "        else:\n",
    "            self.model = model\n",
    "        # Initialize Parameters:\n",
    "        self.members = []\n",
    "        self._nmembers = 0\n",
    "        self.predictions = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        reps = [member.name for member in self.members]\n",
    "        return \"<StackEnsemble: [\" + \", \".join(reps) + \"]>\"\n",
    "\n",
    "    def __str__(self):\n",
    "        reps = [member.name for member in self.members]\n",
    "        return \"StackEnsemble: with\" + str(self._nmembers) + \" Base-Learners [\" + \", \".join(reps) + \"]\"\n",
    "\n",
    "    def add_member(self, member):\n",
    "        \"\"\"\n",
    "        Adds a ensemble Member to the Stack\n",
    "        Args:\n",
    "            member: an instance of class `Member`\n",
    "        \"\"\"\n",
    "        self.members.append(member)\n",
    "        self._nmembers += 1\n",
    "        if member.val_probs is None:\n",
    "            try:\n",
    "                member.val_probs = member._calculate_val_predictions()\n",
    "            except Exception as e:\n",
    "                warnings.warn(str(e))\n",
    "        if member.train_probs is None:\n",
    "            try:\n",
    "                member.train_probs = member._calculate_train_predictions()\n",
    "            except Exception as e:\n",
    "                warnings.warn(str(e))\n",
    "\n",
    "    def fit(self, X=None, y=None, kwargs={}):\n",
    "        \"\"\"\n",
    "        Trains the meta-model\n",
    "        Args:\n",
    "            X: training data for meta-learner\n",
    "            y: training classes for meta-learner\n",
    "            kwargs: further arguments for the fit function\n",
    "        \"\"\"\n",
    "        assert(len(self.members) > 1) \n",
    "        # Assumption: all members have same train_batches.classes\n",
    "        if X is None or y is None:\n",
    "            return self._fit_train()\n",
    "        assert(X.ndim <= 3)\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "        return self.model.fit(X, y, **kwargs)\n",
    "\n",
    "    def predict(self, X=None, predict_proba=False, kwargs={}):\n",
    "        \"\"\"\n",
    "        Meta-Model prediction for the probabilities of the positive class as a regression problem\n",
    "        Args:\n",
    "            X: input data to be predicted\n",
    "            kwargs: further arguments for prediction function\n",
    "            predict_proba: if should call method `predict_proba`instead of `predict`\n",
    "        Returns:\n",
    "            the predicted probabilities as np.array\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            X = self._get_pred_X()\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "        if predict_proba and hasattr(self.model, 'predict_proba'):\n",
    "            self.predictions = self.model.predict_proba(X, **kwargs)\n",
    "        elif hasattr(self.model, 'predict'):\n",
    "            self.predictions = self.model.predict(X, **kwargs)\n",
    "        else:\n",
    "            raise(\"Model has no predict function\")\n",
    "        return np.array(self.predictions)\n",
    "\n",
    "    def describe(self, probabilities_val=None):\n",
    "        \"\"\"\n",
    "        Prints information about the performance of base and meta learners based on validation data\n",
    "        Args:\n",
    "            probabilities_val: (optional) probabilities/prediction on validation data\n",
    "        \"\"\"\n",
    "        modelbestauc = 0\n",
    "        if probabilities_val is None:\n",
    "            probabilities_val = self._predict_val()\n",
    "        val_classes = self.members[0].val_classes  # Assumption: all members have same val_classes\n",
    "        for i in range(self._nmembers):\n",
    "            member = self.members[i]\n",
    "            valprobs = member.val_probs\n",
    "            auc = metrics.roc_auc_score(member.val_classes, valprobs)\n",
    "            if auc > modelbestauc:\n",
    "                modelbestauc = auc\n",
    "            print(member.name, \"AUC:\", auc)\n",
    "        auc = metrics.roc_auc_score(val_classes, probabilities_val)\n",
    "        print(\"Ensemble AUC:\", auc)\n",
    "        return auc\n",
    "\n",
    "    def _get_X(self, attrname):\n",
    "        X = []\n",
    "        probs = getattr(self.members[0], attrname)\n",
    "        for i in range(len(probs)):  # Assumption: all members have same train_probs length\n",
    "            preds = []\n",
    "            for member in self.members:\n",
    "                preds.append(getattr(member, attrname)[i])\n",
    "            X.append(preds)\n",
    "        return np.array(X)\n",
    "\n",
    "    def _get_train_X(self):\n",
    "        return self._get_X(\"train_probs\")\n",
    "\n",
    "    def _get_val_X(self):\n",
    "        return self._get_X(\"val_probs\")\n",
    "\n",
    "    def _get_pred_X(self):\n",
    "        return self._get_X(\"submission_probs\")\n",
    "\n",
    "    def _fit_train(self):\n",
    "        return self.fit(self._get_train_X(), self.members[0].train_classes)\n",
    "\n",
    "    def _fit_submission(self):\n",
    "        \"\"\"\n",
    "        Fits model on training and validation data.\n",
    "        Useful when training the meta-learner for final submission prediction\n",
    "        \"\"\"\n",
    "        X1 = self._get_train_X()\n",
    "        X2 = self._get_val_X()\n",
    "        y1 = self.members[0].train_classes\n",
    "        y2 = self.members[0].val_classes\n",
    "        X = np.concatenate((X1, X2))\n",
    "        y = np.concatenate((y1, y2))\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def _predict_val(self):\n",
    "        return self.predict(self._get_val_X())\n",
    "\n",
    "    def _test(self):\n",
    "        \"\"\"\n",
    "        Test assumption that all members' classes have same shape and values. Names should be unique.\n",
    "        This is an internal condition for class structures.\n",
    "        \"\"\"\n",
    "        if self._nmembers < 2:\n",
    "            return True\n",
    "        t1 = [(_compare_arrays(self.members[i].train_classes,\n",
    "                               self.members[i + 1].train_classes)) for i in range(self._nmembers - 1)]\n",
    "        t2 = [(_compare_arrays(self.members[i].val_classes,\n",
    "                               self.members[i + 1].val_classes)) for i in range(self._nmembers - 1)]\n",
    "        assert(np.sum(t1) == self._nmembers - 1)\n",
    "        assert(np.sum(t2) == self._nmembers - 1)\n",
    "        names = [self.members[i].name for i in range(self._nmembers)]\n",
    "        assert(len(list(names)) == len(set(names)))\n",
    "        return True\n",
    "\n",
    "    def save(self, folder=\"./premodels/\", save_kerasmodel=False):  # TODO: Document\n",
    "        \"\"\"\n",
    "        Saves meta-learner and base-learner of ensemble into folder / directory\n",
    "        Args:\n",
    "            folder: the folder where models should be saved to. Create if not exists.\n",
    "            save_kerasmodel: if members / base-learners should save the keras model as part of object.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        [member.save(folder=folder, save_kerasmodel=save_kerasmodel) for member in self.members]\n",
    "        temp = self.members\n",
    "        self.members = None  # Reset base-learners. These are loaded idependently\n",
    "        self._nmembers = 0\n",
    "        joblib.dump(self, os.path.join(folder, \"stackensemble.joblib\"))\n",
    "        self.members = temp\n",
    "        self._nmembers = len(self.members)\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, folder=\"./premodels/\"):\n",
    "        \"\"\"\n",
    "        Loads meta-learner and base-learners from folder / directory\n",
    "        Args:\n",
    "            folder: directory where models should be loaded from\n",
    "        Returns:\n",
    "            loaded StackEnsemble with Members\n",
    "        \"\"\"\n",
    "        stack = joblib.load(os.path.join(folder, \"stackensemble.joblib\"))\n",
    "        stack.members = []\n",
    "        if folder[-1] != os.sep:\n",
    "            folder += os.sep\n",
    "        for fn in glob.glob(folder + \"**/\"):\n",
    "            member = Member.load(fn)\n",
    "            stack.add_member(member)\n",
    "        return stack\n",
    "\n",
    "\n",
    "def _compare_arrays(a1, a2):\n",
    "    if a1 is None and a2 is None:\n",
    "        return True\n",
    "    if a1 is None and a2 is not None:\n",
    "        return False\n",
    "    if a1 is not None and a2 is None:\n",
    "        return False\n",
    "    c1 = a1.shape == a2.shape\n",
    "    c2 = np.sum(a1 == a2) == len(a1)\n",
    "    return c1 and c2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1=datagen.flow(x_val1, y_val1, batch_size=batch_size)\n",
    "vb1=datagen.flow(x_test1, y_test1, batch_size=batch_size)\n",
    "\n",
    "tb2=datagen.flow(x_val2, y_val2, batch_size=batch_size)\n",
    "vb2=datagen.flow(x_test2, y_test2, batch_size=batch_size)\n",
    "\n",
    "tb3=datagen.flow(x_val3, y_val3, batch_size=batch_size)\n",
    "vb3=datagen.flow(x_test3, y_test3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3) (500, 10)\n",
      "(500, 32, 32, 3) (500, 10)\n",
      "(500, 32, 32, 3) (500, 10)\n",
      "(500, 32, 32, 3) (500, 10)\n",
      "(500, 32, 32, 3) (500, 10)\n",
      "(500, 32, 32, 3) (500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(tb1.x.shape, tb1.y.shape)\n",
    "print(tb2.x.shape, tb2.y.shape)\n",
    "print(tb3.x.shape, tb3.y.shape)\n",
    "print(vb1.x.shape, vb1.y.shape)\n",
    "print(vb2.x.shape, vb2.y.shape)\n",
    "print(vb3.x.shape, vb3.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "16/16 [==============================] - 1s 34ms/step\n",
      "16/16 [==============================] - 1s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "member1 = Member(name=\"model1\", keras_model = model1, \n",
    "                 train_batches = tb1, \n",
    "                 val_batches = vb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 33ms/step\n",
      "16/16 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "member2 = Member(name=\"model2\", keras_model = model2, \n",
    "                 train_batches = tb2, \n",
    "                 val_batches = vb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 30ms/step\n",
      "16/16 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "member3 = Member(name=\"model3\", keras_model = model3, \n",
    "                 train_batches = tb3, \n",
    "                 val_batches = vb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 AUC: 0.8247173671497837\n",
      "model2 AUC: 0.83927852541473\n",
      "model3 AUC: 0.8312110590553639\n",
      "Ensemble AUC: 0.8447110714493997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8447110714493997"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "stack = StackEnsemble()\n",
    "stack.model = RandomForestRegressor(verbose=1, n_estimators=1000, max_depth=7, n_jobs=4)\n",
    "stack.add_member(member1)\n",
    "stack.add_member(member2)\n",
    "stack.add_member(member3)\n",
    "stack.fit()\n",
    "stack.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490796613610122 0 [0.01592542 0.12675684 0.85731774]\n",
      "0.863194468912155 1 [0.33786213 0.47873087 0.183407  ]\n",
      "0.8646659245070655 2 [0.32930661 0.38337132 0.28732207]\n",
      "0.8647314886603343 68 [0.22615976 0.33038236 0.44345788]\n",
      "0.8647901350478067 75 [0.28610422 0.41277567 0.30112011]\n",
      "model1 - Weight: 0.2861042203719244 - Single AUC: 0.8247173671497837\n",
      "model2 - Weight: 0.41277566947446226 - Single AUC: 0.83927852541473\n",
      "model3 - Weight: 0.3011201101536134 - Single AUC: 0.8312110590553639\n",
      "DirichletEnsemble AUC: 0.8647901350478067\n"
     ]
    }
   ],
   "source": [
    "dstack = DirichletEnsemble(N=100)\n",
    "dstack.add_member(member1)\n",
    "dstack.add_member(member2)\n",
    "dstack.add_member(member3)\n",
    "dstack.fit(verbose=True)\n",
    "dstack.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-2af30c39620f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-167-1324f202e493>\u001b[0m in \u001b[0;36m_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m         t2 = [(_compare_arrays(self.members[i].val_classes,\n\u001b[1;32m    415\u001b[0m                                self.members[i + 1].val_classes)) for i in range(self._nmembers - 1)]\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nmembers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nmembers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nmembers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stack._test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [(np.array_equal(stack.members[i].train_classes,\n",
    "                               stack.members[i + 1].train_classes)) for i in range(stack._nmembers - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
